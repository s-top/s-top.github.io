---
layout: post
title: 增强学习
tags: [科普]
---
#### 在没有棋谱或者老师的情况下，怎样才能学会下棋呢？

可以通过尝试随机走棋，然后逐步建立对环境的一个预测模型，就能判断出走出某步后棋盘的形势，然后选择对己方最优的下棋方案。如果赢了，就意味着好的事情发生了；而输了，则意味着事情不妙。这类反馈称为回报。

增强学习的任务就是利用观察到的回报来学习针对某个环境的最优或者接近最优的策略。

在很多复杂领域中，增强学习是对程序进行训练以表现出高性能的唯一可行途径。

* 关键词： --

可以认为增强学习包括了人工智能的所有要素：一个智能体被置于一个环境中，并且必须学会在其间游刃有余。

### 一、智能体设计

#### 1.基于效用的智能体学习关于状态的效用函数，然后使用它选择使得结果的期望效用最大化的行为。

>基于效用的智能体还必须具备环境模型以便进行决策，因为它必须知道其行动将导致的状态。只有如此，它才能将效用函数应用于结果状态。

#### 2.Q学习智能体学习行为-价值函数，或称为Q函数，该函数提供给定状态下采取特定行为的期望效用。

>一个Q学习智能体可以比较各种可能选择的价值，而不必知道其选择会带来的后果，所以它不需要环境模型。另外，因为它们不知道其行动将引向何方，Q学习智能体无法前瞻，将看到这会严重限制它们的学习能力。

#### 3.反射型智能体学习一种策略，直接从状态映射到行为。

### 二、被动增强学习

在完全可观察环境下使用基于状态的表示的被动学习智能体的情形。在被动学习中，智能体的策略π是固定的，处于状态s，它总是执行行动$$\pi(s)$$。其目标是学习效用函数：$$U^π(s)$$

显然，被动学习的任务与策略评估（策略迭代算法的一部分）的任务相似。主要区别在于被动学习智能体对概率转移模型$$T(s,a,s')$$一无所知，并且也不知道状态s的回报函数$$R(s)$$。可以通过大量的在线试验来学习转移模型$$T(s,a,s')$$和回报函数$$R(s)$$

#### 1.直接状态估计

基本思想是：认为一个状态的效用是从该状态开始往后的期望总回报，而每次试验对于每个被访问到的状态提供了该效用值的一个样本。在进行无穷多次实验的极限情况下，样本平均值将收敛于真实期望值。因为采用会以正确的概率出现，所以此方法是可行的。

>进行多组试验，然后对出现的每个样本进行采样，并计算采样值的公式为：

$$sample_k=R(s,π(s),s_k')+γV_i^π(s_k')$$

>最后平均可得效用为：

$$V_{i+1}^π(s)<-\quad\sum_{k}sample_k/k$$

直接效用估计将增强问题简化为归纳学习问题。但是，它忽视了“状态的效用并非相互独立的”这个事实。每个状态的效用等于它自己的回报加上其后继状态的期望效用。就是说，效用值服从[固定策略的贝尔曼方程](https://baike.baidu.com/item/贝尔曼方程)，即：

$$U^π(s)=R(s)+γ\quad\sum_{s'}T(s,\pi(s),s')U^π(s')$$

由于忽略了状态之间的联系，直接效用估计错过了学习的机会。更一般地讲，可以认为直接效用估计在比实际需要大得多的假设空间中搜索U，其中包含了很多违反贝尔曼方程的情形。因此，该算法的收敛速度通常很慢。

#### 2.基于模型的估计

简化的内尔曼更新：

$$V_{i+1}^π(s)=\quad\sum_{s'}T(s,\pi(s),s')[R(s,\pi(s),s')+γV_i^π(s')]$$

$$U^π(s)=R(s)+γ\quad\sum_{s'}T(s,\pi(s),s')U^π(s')$$

此处需要转移模型$$T(s,a,s')$$和回报函数$$R(s)$$

* 计算转移模型：

对每个s，a元祖的每种结果进行计数，试验结束后，得到所有可能的(s,a,s')的计数，这样可以得到$$T(s,a,s')$$的估计。

>对于状态s的期望效用为:

$$U(s)=\quad\sum_{}T(s,a,s')R(s)$$

>通过采样，可以求得状态转移模型的估计为：

$$T'(s,a,s')=Count(s,a,s')/Count(s,a)$$

>因此，状态s的期望效用为：

$$U(s)=\quad\sum_{}R(s)Count(s,a,s')/Count(s,a)$$

为了利用状态间的约束，智能体必须学习状态之间的联系。自适应动态规划（Adaptive Dynamic Program,ADP）智能体通过在运行过程中学习环境的转移模型来工作，并且运用动态规划方法求解相应的马尔科夫决策过程。

对于一个被动学习智能体，这意味着把学到的转移模型和观察到的回报代入简化的贝尔曼方程中，计算状态的效用。

### 二、时序差分学习

几乎同时具备上述两种方法的优点是可能的：

* 一方面满足约束方程
* 一方面也不必对所有可能的状态求解

关键在于使用观察到的转移模型来调整观察到的状态的值，使它们与约束方程相一致。一般来说，当发生从状态s到s'的转移时，将对$$U^π(s)$$应用如下更新：

$$U^π(s)<-U^π(s)+α(R(s)+γU^π(s')-U^π(s))$$

其中，α是学习速度参数。因此此更新规则使用的是相继状态之间的效用差分，它也经常称为时序差分或TD(Temporal Difference)公式。

所以时序差分方法的基本思想是：首先定义当效用评估正确时局部成立的条件，然后写出更新公式使估计值向理想的“均衡”方程靠近。

TD智能体尽管在学习速度上没有ADP智能体快，并且表现出更高的易变性，但是它更简单，每次观察所需的计算量也要少得多。

>时序差分算法事实上给出的是一种指数滑动平均，将上述公式表示为更一般的形式：

$$\overline{x}_n=(1-α)×\overline{x}_{n-1}+x_n$$

>逐次迭代展开后，代入效用更新公式，可得：

$$U_{n+1}(s)\frac{U_n(s)+(1-α)x_{n-1}+(1-α)^2x_{n-2}+\cdots}{1+(1-α)+(1-α)^2+\cdots}$$

因此，TD算法事实上是滑动平均的过程，该算法可以很快地忘记过去的值，而使现在的采样更为重要，并且很容易通过滑动平均计算。在TD的学习中，整个策略评估的过程，都是与模型无关的。整个过程中，采用策略固定的方法，这就是称为被动学习的原因。当被动学习终止以后，可以得到所有状态的效用值，但是却面临一个问题，即无法进行策略的选择。

因为有所有状态的效用值，所以可以按照期望效用最大原则选出最优策略，但是需要转移模型$$T(s,a,s')$$。解决这个问题的方法是直接学习Q值，使行为选择也与模型无关。

>在马尔科夫决策过程中，Q学习公式如下：

$$π(s)=argmaxQ^*(s,a)$$

$$Q^*(s,a)=\quad\sum_{s'}T(s,a,s')[R(s,a,s')+γV^*(s')]$$









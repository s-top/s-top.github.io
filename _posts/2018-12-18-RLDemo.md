---
layout: post
title: 强化学习-学习记录
tags: [科普]
---
#### 强化学习的应用

* 关键词：强化学习--推理

### 强化学习理论

强化学习是指强化学习系统通过与外部环境交互取得状态与行为的映射关系，学习的目标是使得奖励回报函数数值达到最优(最大或最小)。

由于外部环境只能提供很少的信息，强化学习系统必须依靠自身的经验进行自主学习。

强化学习是从试错学习、参数扰动自适应控制、时序差分等理论发展而来。

#### 马尔科夫决策过程(MDP,markov decision process)

状态和行为的空间是有限的马尔科夫决策过程，称为有限马尔科夫决策过程。

马尔科夫决策过程的本质：当前状态向下一状态转移的概率和奖励值与历史状态和历史行为无关，只取决于当前状态和当前选择的行为。

+ 马尔科夫决策过程：指决策者连续地观察具有马尔科夫性质、依次作出决策的随机过程。

+ 策略：智能学习系统与环境交互过程中选择行为的方法。

+ 最优策略：所有策略中最优的，可以通过评价函数来得到。

#### Q学习算法

+ 步骤
+ 期望回报函数
    + 有限域模型
    + 无线域折扣模型
    + 有限域折扣模型
+ 状态-行为对的Q值函数
+ 行为选择机制
    + 所谓行为选择机制，就是如何与环境进行有效地交互，在强化学习领域中，体现在智能系统怎样根据实时感知到的环境状态迅速地选择行为并执行。

在环境模型(状态转移概率函数和奖励函数)已知的情况下，可以对MDP模型采用动态规划的方法进行求解。

在现实世界中，T和R往往无法获得。此时，就需要智能系统与环境进行有效地交互，并从交互结果中获取经验来学习最优Q值函数。

> 行为选择机制的设计在学习过程中药同时考虑两个相互矛盾的因素：利用和探索

greedy机制、Boltzmann分布机制：通过给每个行为赋予一定的选择比重来实现对全部行为的有效尝试。

pursuit函数机制：当运行t个周期后在t+1周期时选择最优行为的概率

动态规划算法，“建模灾难”和“维数灾难”。

蒙特卡洛算法，通过样本回报的平均化来解决强化学习问题的方法，该方法将需要解决的问题分解成不同的子问题，并对值函数进行评估从而找到最优策略。

瞬时差分学习算法

SARSA学习算法

Dyna学习算法

AHC学习算法

+ TD学习算法
    + Actor-Critic方法
    + R学习算法

#### 离线Q学习xxx模型的构建

先建立交通控制问题的模型，再利用离线学习算法对各种交通状态及配时方案进行学习，从而得到不同交通状态下的最优配时方案，最后将最优配时方案应用到实际交叉口的交通信号控制中。

==================================分割线==================================

### 强化学习随笔

#### Policy Gradients

基本思想：就是直接根据状态输出动作或者动作的概率。

使用神经网络输入当前的状态，网络就可以输出我们在这个状态下采取每个动作的概率，那么网络应该如何训练来实现最终的收敛呢？

对于强化学习来说，我们不知道动作的正确与否，只能通过奖励值来判断这个动作的相对好坏。

> 如果一个动作得到的reward多，那么我们就使其出现的概率增加；如果一个动作得到的reward少，我们就使其出现的概率减小。

+ Policy Gradient的核心思想是更新参数时有两个考虑：
    + 如果这个回合选择某一动作，下一回合选择该动作的概率大一些，然后再看奖惩值
        + 如果奖惩是正的，那么会放大这个动作的概率
        + 如果奖惩是负的，就会减小该动作的概率

> 策略梯度的过程如下图所示：

![image]({{ site.baseurl }}/assets/img/blog/2018-12-18-RLDemo/1.png)

#### DDPG

> DDPG算法流程：

![image]({{ site.baseurl }}/assets/img/blog/2018-12-18-RLDemo/2.png)

#### MADDPG

> MADDPG算法流程：

![image]({{ site.baseurl }}/assets/img/blog/2018-12-18-RLDemo/3.png)

MADDPG算法具有以下三点特征：

1.通过学习得到的最优策略，在应用时只利用局部信息就能给出最优动作。

2.不需要知道环境的动力学模型以及特殊的通信需求。

3.该算法不仅能用于合作环境，也能用于竞争环境。

MADDPG算法具有以下三点技巧：

集中式训练，分布式执行：训练时采用集中式学习训练critic与actor，使用时actor只用知道局部信息就能运行。critic需要其他智能体的策略信息，本文给了一种估计其他智能体策略的方法，能够只用知道其他智能体的观测与动作。

改进了经验回放记录的数据。为了能够适用于动态环境，每一条信息由 (x,x', a_q,\cdots,a_n,r_1,\cdots,r_n) 组成， x=(o_1,\cdots,o_n) 表示每个智能体的观测。

利用策略集合效果优化（policy ensemble）：对每个智能体学习多个策略，改进时利用所有策略的整体效果进行优化。以提高算法的稳定性以及鲁棒性。

集中训练：在训练时，我们可以在Critic阶段加上一些额外的信息来得到更准确的Q值，比如其他智能体的状态和动作等，这也就是集中训练的意思，即每个智能体不仅仅根据自身的情况，还根据其他智能体的行为来评估当前动作的价值。

分散执行：当每个Agent都训练充分之后，每个Actor就可以自己根据状态采取合适的动作，此时是不需要其他智能体的状态或者动作的。

注：参考资料(https://blog.csdn.net/WASEFADG/article/details/83240034)

注：论文翻译(https://blog.csdn.net/qiusuoxiaozi/article/details/79066612)

注：参考资料(https://zhuanlan.zhihu.com/p/53811876)

注：参考资料(https://blog.openai.com/learning-to-cooperate-compete-and-communicate/)


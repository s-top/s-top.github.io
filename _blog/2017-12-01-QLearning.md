---
layout: post
title: 增强学习
tags: [科普]
---
#### 在没有棋谱或者老师的情况下，怎样才能学会下棋呢？

可以通过尝试随机走棋，然后逐步建立对环境的一个预测模型，就能判断出走出某步后棋盘的形势，然后选择对己方最优的下棋方案。如果赢了，就意味着好的事情发生了；而输了，则意味着事情不妙。这类反馈称为回报。

增强学习的任务就是利用观察到的回报来学习针对某个环境的最优或者接近最优的策略。

在很多复杂领域中，增强学习是对程序进行训练以表现出高性能的唯一可行途径。

* 关键词：被动增强学习 -- 时序差分学习 -- 主动增强学习

可以认为增强学习包括了人工智能的所有要素：一个智能体被置于一个环境中，并且必须学会在其间游刃有余。

### 一、智能体设计

#### 1.基于效用的智能体学习关于状态的效用函数，然后使用它选择使得结果的期望效用最大化的行为。

>基于效用的智能体还必须具备环境模型以便进行决策，因为它必须知道其行动将导致的状态。只有如此，它才能将效用函数应用于结果状态。

#### 2.Q学习智能体学习行为-价值函数，或称为Q函数，该函数提供给定状态下采取特定行为的期望效用。

>一个Q学习智能体可以比较各种可能选择的价值，而不必知道其选择会带来的后果，所以它不需要环境模型。另外，因为它们不知道其行动将引向何方，Q学习智能体无法前瞻，将看到这会严重限制它们的学习能力。

#### 3.反射型智能体学习一种策略，直接从状态映射到行为。

### 二、被动增强学习

在完全可观察环境下使用基于状态的表示的被动学习智能体的情形。在被动学习中，智能体的策略π是固定的，处于状态s，它总是执行行动$$\pi(s)$$。其目标是学习效用函数：$$U^π(s)$$

显然，被动学习的任务与策略评估（策略迭代算法的一部分）的任务相似。主要区别在于被动学习智能体对概率转移模型$$T(s,a,s')$$一无所知，并且也不知道状态s的回报函数$$R(s)$$。可以通过大量的在线试验来学习转移模型$$T(s,a,s')$$和回报函数$$R(s)$$

#### 1.直接状态估计

基本思想是：认为一个状态的效用是从该状态开始往后的期望总回报，而每次试验对于每个被访问到的状态提供了该效用值的一个样本。在进行无穷多次实验的极限情况下，样本平均值将收敛于真实期望值。因为采用会以正确的概率出现，所以此方法是可行的。

>进行多组试验，然后对出现的每个样本进行采样，并计算采样值的公式为：

$$sample_k=R(s,π(s),s_k')+γV_i^π(s_k')$$

>最后平均可得效用为：

$$V_{i+1}^π(s)=\quad\sum_{k}sample_k/k$$

直接效用估计将增强问题简化为归纳学习问题。但是，它忽视了“状态的效用并非相互独立的”这个事实。每个状态的效用等于它自己的回报加上其后继状态的期望效用。就是说，效用值服从[固定策略的贝尔曼方程](https://baike.baidu.com/item/贝尔曼方程)，即：

$$U^π(s)=R(s)+γ\quad\sum_{s'}T(s,\pi(s),s')U^π(s')$$

由于忽略了状态之间的联系，直接效用估计错过了学习的机会。更一般地讲，可以认为直接效用估计在比实际需要大得多的假设空间中搜索U，其中包含了很多违反贝尔曼方程的情形。因此，该算法的收敛速度通常很慢。

#### 2.基于模型的估计

简化的内尔曼更新：

$$V_{i+1}^π(s)=\quad\sum_{s'}T(s,\pi(s),s')[R(s,\pi(s),s')+γV_i^π(s')]$$

此处需要转移模型$$T(s,a,s')$$和回报函数$$R(s)$$

* 计算转移模型：

对每个s，a元祖的每种结果进行计数，试验结束后，得到所有可能的(s,a,s')的计数，这样可以得到$$T(s,a,s')$$的估计。

>对于状态s的期望效用为:

$$U(s)=\quad\sum_{}T(s,a,s')R(s)$$

>通过采样，可以求得状态转移模型的估计为：

$$T'(s,a,s')=Count(s,a,s')/Count(s,a)$$

>因此，状态s的期望效用为：

$$U(s)=\quad\sum_{}R(s)Count(s,a,s')/Count(s,a)$$

为了利用状态间的约束，智能体必须学习状态之间的联系。自适应动态规划（Adaptive Dynamic Program,ADP）智能体通过在运行过程中学习环境的转移模型来工作，并且运用动态规划方法求解相应的马尔科夫决策过程。

对于一个被动学习智能体，这意味着把学到的转移模型和观察到的回报代入简化的贝尔曼方程中，计算状态的效用。

### 三、时序差分学习

几乎同时具备上述两种方法的优点是可能的：

* 一方面满足约束方程
* 一方面也不必对所有可能的状态求解

关键在于使用观察到的转移模型来调整观察到的状态的值，使它们与约束方程相一致。一般来说，当发生从状态s到s'的转移时，将对$$U^π(s)$$应用如下更新：

$$U^π(s)=U^π(s)+α(R(s)+γU^π(s')-U^π(s))$$

其中，α是学习速度参数。因此此更新规则使用的是相继状态之间的效用差分，它也经常称为时序差分或TD(Temporal Difference)公式。

所以时序差分方法的基本思想是：首先定义当效用评估正确时局部成立的条件，然后写出更新公式使估计值向理想的“均衡”方程靠近。

TD智能体尽管在学习速度上没有ADP智能体快，并且表现出更高的易变性，但是它更简单，每次观察所需的计算量也要少得多。

>时序差分算法事实上给出的是一种指数滑动平均，将上述公式表示为更一般的形式：

$$\overline{x}_n=(1-α)×\overline{x}_{n-1}+x_n$$

>逐次迭代展开后，代入效用更新公式，可得：

$$U_{n+1}(s)=\frac{U_n(s)+(1-α)U_{n-1}(s)+(1-α)^2U_{n-2}(S)+\cdots}{1+(1-α)+(1-α)^2+\cdots}$$

因此，TD算法事实上是滑动平均的过程，该算法可以很快地忘记过去的值，而使现在的采样更为重要，并且很容易通过滑动平均计算。在TD的学习中，整个策略评估的过程，都是与模型无关的。整个过程中，采用策略固定的方法，这就是称为被动学习的原因。当被动学习终止以后，可以得到所有状态的效用值，但是却面临一个问题，即无法进行策略的选择。

因为有所有状态的效用值，所以可以按照期望效用最大原则选出最优策略，但是需要转移模型$$T(s,a,s')$$。解决这个问题的方法是直接学习Q值，使行为选择也与模型无关。

>在马尔科夫决策过程中，Q学习公式如下：

$$π(s)=argmaxQ^*(s,a)$$

$$Q^*(s,a)=\quad\sum_{s'}T(s,a,s')[R(s,a,s')+γV^*(s')]$$

通过存储$$Q(s,a)$$，表示在状态s处采用动作a的期望效用值。而在状态s处选择所有行为中期望效用最大的行为，就可以得到状态s的策略。因此，直接学习Q值，可使行为选择也与模型无关。

* ADP与TD相比较

ADP方法与TD方法实际上是密切相关的，二者都试图对效用估计进行局部调整，以使每一个状态都与其后继状态相“一致”。

一个区别在于TD调整一个状态使其与已观察到的后继状态相一致，而ADP则调整该状态使其与所有可能出现的后继状态相一致，根据概率进行加权。

而更重要的差别是，TD对每个观察到的转移都只进行单一的调整，而ADP为了重建效用估计U和环境模型T之间的一致性会按所需进行尽可能多的调整。虽然观察到的转移只造成T的局部变化，其影响却可能需要在整个U中传递。因此，TD可以视为对ADP的一个粗略而有效的一阶近似。

### 四、主动增强学习

决定被动学习智能体行为表现的策略是固定的。主动智能体必须自行决定采取什么行动。首先，智能体将需要学习一个包含所有行为结果概率的完整模型，接下来，需要允许智能体选择行为。

>它需要学习的效用是由最优策略定义的，这些效用遵守内尔曼方程：

$$U(s)=R(s)+γmax\quad\sum_{s'}T(s,a,s')U(s')$$

可以运用值迭代或策略迭代算法求解这些方程并获得效用函数U。在学习到一个模型后，可得到对于此模型最优的效用函数U，然后智能体前瞻一步，使用期望效用最大化得到一个最优行为。而如果使用策略迭代，最优策略已得到了，所以它应该简单地执行最优策略所建议的行为。

#### 1.探索与利用

但是它应该这样做吗？答案是否定的!

在学习过程中，每次都选择最优行为时将得到非最优结果，其原因在于学习到的模型与真实环境并不相同。

为了得到真实的模型，必须对上述贪婪智能体的行为进行改进。基本思路是：在探索信息和利用信息（认为当前获得的模型是正确的，按此模型进行进一步的行动）之间进行折中，而探索则忽略当前学到的模型，进行随机行动。一种简单的折中就是：让智能体在某一实践片段内选择一个随机行动，其他时间遵循贪婪策略，收敛速度会很慢。另一种方法就是：对行为加权，即较少试验的行为，给予更高的权重，而同时注意避免那些已知具有低效用的行为。

#### 2.学习Q值函数

除了通过学习得到的效用值，还可以直接学习Q值。一种做法是基于Q学习的时序差分方法，它学习的是一种行动-值表示而非效用。用符号$$Q(a,s)$$代表在状态s采取行动a的值。

>如下所示，Q值与效用值直接相关：

$$U(s)=maxQ(a,s)$$

Q函数不仅可用来表示效用信息，更重要的是，其不需要一个用于学习或行为选择的模型。因此，Q学习是一种无模型方法：

>时序差分Q学习更新公式为：

$$Q(a,s)=Q(a,s)+α(R(s)+γmaxQ(a',s')-Q(a,s))$$

只要在状态s下执行行为α导致了状态s'，就对其进行计算。

### 五.增强学习的一般化

目前为止，一直假定智能体学习到的效用函数和Q函数是通过每个输入对应一个输出值的表格形式表示的。对于小规模的状态空间，这种方法很起作用，但是随着空间的增大，收敛的时间和每次迭代的时间都会迅速增加。

处理大规模的状态空间问题的一个方法是：应用函数逼近：

>加权函数表示：

$$U_θ(s)=θ_1f_1(s)+θ_2f_2(s)+\cdots+θ_nf_n(s)$$

对于增强学习，应用一种在每次试验后都对参数进行更新的联机学习算法则更有意义。

对于神经网络，可写一个误差函数并计算它关于参数的梯度，误差就定义为预测总回报和实际总回报的（一半）方差。

>如果$$u_j(s)$$是第j次试验中从状态s开始学习到的总回报，那么误差为：

$$E_j(s)=\frac{(U_θ(s)-u_j(s))^2}{2}$$

该误差关于每个参数$$θ_i$$的变化率是$$\frac{dE_j}{dθ_i}$$，于是为了让参数向减小误差的方向移动，需要：

>以下称为在线最小方差的Widrow-Hoff规划，或称为δ	规则

$$θ_i=θ_i-α\frac{dE_j(s)}{dθ_i}=θ_i+α(u_j(s)-U_θ(s))\frac{dU_θ(s)}{dθ_i}$$

>可得到三条简单的更新规则：

$$θ_0=θ_0+α(u_j(s)-U_θ(s))$$

$$θ_1=θ_1+α(u_j(s)-U_θ(s))x$$

$$θ_2=θ_2+α(u_j(s)-U_θ(s))y$$

#### "函数逼近允许一个增强学习者根据其经验进行一般化"。










